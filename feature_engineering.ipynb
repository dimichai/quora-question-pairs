{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_feature_engineering.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"IcKdWP8EbNR8","colab_type":"text"},"cell_type":"markdown","source":["### Feature Engineering"]},{"metadata":{"id":"Qj1bXWkbXeTm","colab_type":"code","outputId":"d115a6c1-a625-49c3-a854-ad9124053dfe","executionInfo":{"status":"ok","timestamp":1544720046646,"user_tz":-60,"elapsed":189611,"user":{"displayName":"Dimitris Michailidis","photoUrl":"https://lh4.googleusercontent.com/-gqfyDKSRdb4/AAAAAAAAAAI/AAAAAAAAAjM/8OOFAsZoU-M/s64/photo.jpg","userId":"12419194121595599642"}},"colab":{"base_uri":"https://localhost:8080/","height":428}},"cell_type":"code","source":["!pip install fuzzywuzzy\n","!pip install python-Levenshtein\n","\n","!pip install gensim\n","!pip install pyemd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.6/dist-packages (0.17.0)\n","Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.6/dist-packages (0.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (40.6.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n","Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n","Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.62)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.11.29)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.62 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.62)\n","Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n","Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.62->boto3->smart-open>=1.2.1->gensim) (0.14)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.62->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n","Requirement already satisfied: pyemd in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pyemd) (1.14.6)\n"],"name":"stdout"}]},{"metadata":{"id":"1gU8H7l7epBN","colab_type":"code","outputId":"1451e698-bedc-4132-b53b-5ad8b1cd9b72","executionInfo":{"status":"ok","timestamp":1544720086401,"user_tz":-60,"elapsed":219332,"user":{"displayName":"Dimitris Michailidis","photoUrl":"https://lh4.googleusercontent.com/-gqfyDKSRdb4/AAAAAAAAAAI/AAAAAAAAAjM/8OOFAsZoU-M/s64/photo.jpg","userId":"12419194121595599642"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["!wget 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2018-12-13 16:54:07--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.224.195\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.224.195|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1647046227 (1.5G) [application/x-gzip]\n","Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n","\n","GoogleNews-vectors- 100%[===================>]   1.53G  43.2MB/s    in 38s     \n","\n","2018-12-13 16:54:45 (41.8 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"RJ5CUIILXDQ7","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from fuzzywuzzy import fuzz\n","import nltk\n","from nltk import ngrams, bigrams\n","import gensim\n","from nltk.corpus import stopwords\n","from collections import Counter\n","\n","from functools import partial\n","from nltk import word_tokenize\n","from scipy.stats import skew, kurtosis\n","from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RakbWJcuq_gr","colab_type":"code","outputId":"b97df590-ff69-4db5-c2c6-c72a3a71e678","executionInfo":{"status":"ok","timestamp":1544723111196,"user_tz":-60,"elapsed":1037,"user":{"displayName":"Dimitris Michailidis","photoUrl":"https://lh4.googleusercontent.com/-gqfyDKSRdb4/AAAAAAAAAAI/AAAAAAAAAjM/8OOFAsZoU-M/s64/photo.jpg","userId":"12419194121595599642"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":23}]},{"metadata":{"id":"och0hylQaD-1","colab_type":"code","colab":{}},"cell_type":"code","source":["train_df = pd.read_csv('cleaned_train_data.csv')\n","test_df = pd.read_csv('cleaned_test_data.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MlU7XAxaYAy7","colab_type":"text"},"cell_type":"markdown","source":["Create feature dataframes"]},{"metadata":{"id":"S9TaauWTYCf9","colab_type":"code","colab":{}},"cell_type":"code","source":["train_features = pd.DataFrame()\n","test_features = pd.DataFrame()\n","\n","train_features['id'] = train_df['id']\n","test_features['test_id'] = test_df['test_id']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gyi7F9JqX4kt","colab_type":"text"},"cell_type":"markdown","source":["## Basic Features"]},{"metadata":{"id":"Au-H8uckX3nC","colab_type":"code","colab":{}},"cell_type":"code","source":["train_features['len_q1'] = train_df['question1'].apply(lambda x: len(str(x)))\n","train_features['len_q2'] = train_df['question2'].apply(lambda x: len(str(x)))\n","train_features['len_diff'] = train_features.len_q1 - train_features.len_q2\n","train_features['len_char_q1'] = train_df['question1'].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n","train_features['len_char_q2'] = train_df['question2'].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n","train_features['len_char_diff'] = train_features.len_char_q1 - train_features.len_char_q2\n","train_features['len_word_q1'] = train_df['question1'].apply(lambda x: len(str(x).split()))\n","train_features['len_word_q2'] = train_df['question2'].apply(lambda x: len(str(x).split()))\n","train_features['intersection'] = train_df.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n","\n","train_features['avg_wordlength_q1'] = train_features['len_char_q1'] / train_features['len_word_q1']\n","train_features['avg_wordlength_q2'] = train_features['len_char_q2'] / train_features['len_word_q2']\n","train_features['avg_worddiff'] = abs(train_features['avg_wordlength_q1'] - train_features['avg_wordlength_q2'])\n","\n","train_features['dup_q1'] = train_df['question1'].duplicated()\n","train_features['dup_q2'] = train_df['question2'].duplicated()\n","train_features['dup_q1q2'] = train_features['dup_q1'] & train_features['dup_q2']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bvKbD0n0FVFp","colab_type":"code","colab":{}},"cell_type":"code","source":["test_features['len_q1'] = test_df['question1'].apply(lambda x: len(str(x)))\n","test_features['len_q2'] = test_df['question2'].apply(lambda x: len(str(x)))\n","test_features['len_diff'] = test_features.len_q1 - test_features.len_q2\n","test_features['len_char_q1'] = test_df['question1'].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n","test_features['len_char_q2'] = test_df['question2'].apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n","test_features['len_char_diff'] = test_features.len_char_q1 - test_features.len_char_q2\n","test_features['len_word_q1'] = test_df['question1'].apply(lambda x: len(str(x).split()))\n","test_features['len_word_q2'] = test_df['question2'].apply(lambda x: len(str(x).split()))\n","test_features['intersection'] = test_df.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n","\n","test_features['avg_wordlength_q1'] = test_features['len_char_q1'] / test_features['len_word_q1']\n","test_features['avg_wordlength_q2'] = test_features['len_char_q2'] / test_features['len_word_q2']\n","test_features['avg_worddiff'] = abs(test_features['avg_wordlength_q1'] - test_features['avg_wordlength_q2'])\n","\n","test_features['dup_q1'] = test_df['question1'].duplicated()\n","test_features['dup_q2'] = test_df['question2'].duplicated()\n","test_features['dup_q1q2'] = test_features['dup_q1'] & test_features['dup_q2']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4YgJOx36Wx8T","colab_type":"text"},"cell_type":"markdown","source":["## Fuzzywuzzy Features"]},{"metadata":{"id":"ImHqBV8PW0pM","colab_type":"code","colab":{}},"cell_type":"code","source":["train_features['fuzz_qratio'] = train_df.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n","train_features['fuzz_WRatio'] = train_df.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n","train_features['fuzz_partial_ratio'] = train_df.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","train_features['fuzz_partial_token_set_ratio'] = train_df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","train_features['fuzz_partial_token_sort_ratio'] = train_df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","train_features['fuzz_token_set_ratio'] = train_df.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","train_features['fuzz_token_sort_ratio'] = train_df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h7rfIQ0DYNRl","colab_type":"code","colab":{}},"cell_type":"code","source":["test_features['fuzz_qratio'] = test_df.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n","test_features['fuzz_WRatio'] = test_df.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n","test_features['fuzz_partial_ratio'] = test_df.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","test_features['fuzz_partial_token_set_ratio'] = test_df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","test_features['fuzz_partial_token_sort_ratio'] = test_df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","test_features['fuzz_token_set_ratio'] = test_df.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","test_features['fuzz_token_sort_ratio'] = test_df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2FLufEPL2rFD","colab_type":"code","colab":{}},"cell_type":"code","source":["train_features.to_csv('final_train_basic_fuzz.csv', index=False)\n","test_features.to_csv('final_test_basic_fuzz.csv', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3Nn6yfsMGFwa","colab_type":"text"},"cell_type":"markdown","source":["## Other Features"]},{"metadata":{"id":"ZqiaO_ZrGE40","colab_type":"code","colab":{}},"cell_type":"code","source":["# Is the first word the same?\n","def shared_first_word(entry):\n","    q1_fw = entry['question1'].apply(lambda x: str(x).split()[0])\n","    q2_fw = entry['question2'].apply(lambda x: str(x).split()[0])\n","    \n","    return q1_fw == q2_fw\n","  \n","# Is the last word the same?\n","def shared_last_word(entry):\n","    q1_lw = entry['question1'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n","    q2_lw = entry['question2'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n","\n","    q1_lw = entry['question1'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n","    q2_lw = entry['question2'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n","    \n","    return q1_lw == q2_lw\n","\n","# Is both the first and last word the same?\n","def shared_first_last(entry):\n","    q1_fw = entry['question1'].apply(lambda x: str(x).split()[0])\n","    q2_fw = entry['question2'].apply(lambda x: str(x).split()[0])\n","    same_fw = q1_fw == q2_fw\n","    \n","    q1_lw = entry['question1'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n","    q2_lw = entry['question2'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n","    same_lw = q1_lw == q2_lw\n","    \n","    return same_fw & same_lw\n","\n","# Do the questions have the same content?\n","def same_question(entry):\n","    q1 = entry['question1'].apply(lambda x: str(x).split())\n","    q2 = entry['question2'].apply(lambda x: str(x).split())\n","    \n","    return q1 == q2\n"," \n","# Intersection of twograms in the questions\n","def shared_2grams(entry):\n","    q1 = list(bigrams(str(entry['question1']).lower().split()))\n","    q2 = list(bigrams(str(entry['question2']).lower().split()))\n","    \n","    return len(set(q1).intersection(set(q2)))\n","\n","# Get amount of shared 3-length ngrams in questions\n","def shared_3grams(entry):\n","    q1 = list(ngrams(str(entry['question1']).lower().split(), 3))\n","    q2 = list(ngrams(str(entry['question2']).lower().split(), 3))\n","    \n","    return len(set(q1).intersection(set(q2)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ePGNlkcmaL2A","colab_type":"code","colab":{}},"cell_type":"code","source":["train_features['shared_first_word'] = shared_first_word(train_df)\n","train_features['shared_last_word'] = shared_last_word(train_df)\n","train_features['shared_first_last'] = shared_first_last(train_df)\n","train_features['same_question'] = same_question(train_df)\n","train_features['shared_2grams'] = shared_2grams(train_df)\n","train_features['shared_3grams'] = shared_3grams(train_df)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p9CEKgCMeT-T","colab_type":"code","colab":{}},"cell_type":"code","source":["test_features['shared_first_word'] = shared_first_word(test_df)\n","test_features['shared_last_word'] = shared_last_word(test_df)\n","test_features['shared_first_last'] = shared_first_last(test_df)\n","test_features['same_question'] = same_question(test_df)\n","test_features['shared_2grams'] = shared_2grams(test_df)\n","test_features['shared_3grams'] = shared_3grams(test_df)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HtkJo6lQfmlb","colab_type":"code","outputId":"307e4049-2d0d-46f8-c9c0-2718548c1c42","executionInfo":{"status":"ok","timestamp":1544722199912,"user_tz":-60,"elapsed":484,"user":{"displayName":"Dimitris Michailidis","photoUrl":"https://lh4.googleusercontent.com/-gqfyDKSRdb4/AAAAAAAAAAI/AAAAAAAAAjM/8OOFAsZoU-M/s64/photo.jpg","userId":"12419194121595599642"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["train_features.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(323053, 28)"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"wUIvyBtjfnro","colab_type":"code","outputId":"6ff22e87-8f2a-4af3-cd28-87606f2c5709","executionInfo":{"status":"ok","timestamp":1544722202461,"user_tz":-60,"elapsed":480,"user":{"displayName":"Dimitris Michailidis","photoUrl":"https://lh4.googleusercontent.com/-gqfyDKSRdb4/AAAAAAAAAAI/AAAAAAAAAjM/8OOFAsZoU-M/s64/photo.jpg","userId":"12419194121595599642"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["test_features.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(81126, 28)"]},"metadata":{"tags":[]},"execution_count":17}]},{"metadata":{"id":"EqK831oVmjCo","colab_type":"code","colab":{}},"cell_type":"code","source":["train_features.to_csv('final_train_basic_fuzz.csv', index=False)\n","test_features.to_csv('final_test_basic_fuzz.csv', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aQM-e057XBmw","colab_type":"text"},"cell_type":"markdown","source":["## TF IDF Features"]},{"metadata":{"id":"4HVzdYUBb3Dd","colab_type":"code","colab":{}},"cell_type":"code","source":["# Note: this code is not ours, due to lack of time we had to borrow it from:\n","# https://github.com/HouJP/kaggle-quora-question-pairs\n","\n","class FeatureCreator(object):\n","  \n","    def __init__(self, df, q1_column='question1', q2_column='question2'):\n","        self.df = df\n","        self.q1_column = q1_column\n","        self.q2_column = q2_column\n","        self.stop_words = set(stopwords.words('english'))\n","        self.w2c_model = None\n","\n","    def add_additional_features(self):\n","        \"\"\"Compute TF-IDF and some other interesting features\n","        \"\"\"\n","        self.df['q1_words'] = self.df[self.q1_column].map(lambda x: str(x).lower().split())\n","        self.df['q2_words'] = self.df[self.q2_column].map(lambda x: str(x).lower().split())\n","        questions = pd.Series(self.df['q1_words'].tolist() + self.df['q2_words'].tolist())\n","        words = [word for question in questions for word in question]\n","        word_count = Counter(words)\n","        weights = {word: self.compute_weight(count) for word, count in word_count.items()}\n","        \n","        # Add features\n","        self.df['tfidf_word_match'] = self.df.apply(\n","            partial(self.tfidf_word_match_share, weights=weights, ignore_stop_words=True), axis=1, raw=True\n","        )\n","        self.df['tfidf_word_match_stops'] = self.df.apply(\n","            partial(self.tfidf_word_match_share, weights=weights, ignore_stop_words=False), axis=1, raw=True\n","        )\n","        self.df['jaccard_similarity'] = self.df.apply(partial(self.jaccard_similarity), axis=1, raw=True)\n","        self.df['word_count_ratio'] = self.df.apply(partial(self.word_count_ratio), axis=1, raw=True)\n","        self.df['unique_word_count_ratio'] = self.df.apply(partial(self.unique_word_count_ratio), axis=1, raw=True)\n","        self.df['total_unique_words'] = self.df.apply(partial(self.total_unique_words), axis=1, raw=True)\n","        # Remove columns used for calculations\n","        self.df.drop('q1_words', axis=1, inplace=True)\n","        self.df.drop('q2_words', axis=1, inplace=True)\n","\n","    @staticmethod\n","    def compute_weight(count, epsilon=10000, min_count=2):\n","        if count < min_count:\n","            return .0\n","        else:\n","            return 1.0 / (count + epsilon)\n","\n","    @staticmethod\n","    def unique_word_count_ratio(row):\n","        l1 = float(len(set(row['q1_words'])))\n","        l2 = len(set(row['q2_words']))\n","        if l2 == 0:\n","            return np.nan\n","        if l1 / l2:\n","            return l2 / l1\n","        else:\n","            return l1 / l2\n","\n","    def tfidf_word_match_share(self, row, weights, ignore_stop_words):\n","        q1_words = set()\n","        q2_words = set()\n","        if ignore_stop_words:\n","            for word in row['q1_words']:\n","                q1_words.add(word)\n","            for word in row['q2_words']:\n","                q2_words.add(word)\n","        else:\n","            for word in row['q1_words']:\n","                if word not in self.stop_words:\n","                    q1_words.add(word)\n","            for word in row['q2_words']:\n","                if word not in self.stop_words:\n","                    q2_words.add(word)\n","        if not q1_words or not q2_words:\n","            return .0\n","        shared_weights = np.sum([weights.get(word, .0) for word in q1_words if word in q2_words])\n","        shared_weights += np.sum([weights.get(word, .0) for word in q2_words if word in q1_words])\n","        total_weights = np.sum([weights.get(word, .0) for word in q1_words])\n","        total_weights += np.sum([weights.get(word, .0) for word in q2_words])\n","        return shared_weights / total_weights\n","\n","    @staticmethod\n","    def jaccard_similarity(row):\n","        words_in_common = set(row['q1_words']).intersection(set(row['q2_words']))\n","        unique_words = set(row['q1_words']).union(row['q2_words'])\n","        if not unique_words:\n","            return 1.0\n","        return len(words_in_common) / float(len(unique_words))\n","\n","    @staticmethod\n","    def word_count_ratio(row):\n","        l1 = float(len(row['q1_words']))\n","        l2 = len(row['q2_words'])\n","        if l2 == 0:\n","            return np.nan\n","        if l1 / l2:\n","            return l2 / l1\n","        else:\n","            return l1 / l2\n","\n","    @staticmethod\n","    def total_unique_words(row):\n","        return len(set(row['q1_words']).union(row['q2_words']))\n","\n","    def add_word2vec_features(self, model_path, model_name='w2v', vector_size=300):\n","        \"\"\" word2vec features require a lot of RAM to be computed\n","        \"\"\"\n","        # Load model and compute Word Mover's Distance\n","        self.w2c_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n","        self.w2c_model.init_sims(replace=True)\n","        self.df['{}_norm_wmd'.format(model_name)] = self.df.apply(\n","            lambda x: self.word_mover_distance(x['question1'], x['question2']), axis=1\n","        )\n","        self.w2c_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n","        self.df['{}_wmd'.format(model_name)] = self.df.apply(\n","            lambda x: self.word_mover_distance(x['question1'], x['question2']), axis=1\n","        )\n","        # Generate vectors from questions\n","        question1_vectors = np.zeros((self.df.shape[0], vector_size))\n","        question2_vectors = np.zeros((self.df.shape[0], vector_size))\n","        j = 0\n","        for i, row in self.df.iterrows():\n","            question1_vectors[j, :] = self.text2vec(row[self.q1_column])\n","            question2_vectors[j, :] = self.text2vec(row[self.q2_column])\n","            j += 1\n","        self.w2c_model = None  # Save up some RAM\n","        # Compute several features using vectors\n","        self.df['{}_cosine_distance'.format(model_name)] = [\n","            cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))\n","        ]\n","        self.df['{}_cityblock_distance'.format(model_name)] = [\n","            cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))\n","        ]\n","        self.df['{}_jaccard_distance'.format(model_name)] = [\n","            jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))\n","        ]\n","        self.df['{}_canberra_distance'.format(model_name)] = [\n","            canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))\n","        ]\n","        self.df['{}_euclidean_distance'.format(model_name)] = [\n","            euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))\n","        ]\n","        self.df['{}_minkowski_distance'.format(model_name)] = [\n","            minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))\n","        ]\n","        self.df['{}_braycurtis_distance'.format(model_name)] = [\n","            braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))\n","        ]\n","        self.df['{}_skew_q1vec'.format(model_name)] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n","        self.df['{}_skew_q2vec'.format(model_name)] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n","        self.df['{}_kur_q1vec'.format(model_name)] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n","        self.df['{}_kur_q2vec'.format(model_name)] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n","\n","        \n","    def word_mover_distance(self, text1, text2):\n","        text1 = [w for w in str(text1).lower().split() if w not in self.stop_words]\n","        text2 = [w for w in str(text2).lower().split() if w not in self.stop_words]\n","        return self.w2c_model.wmdistance(text1, text2)\n","\n","    def text2vec(self, text):\n","        text = word_tokenize(str(text).lower())\n","        text = [w for w in text if w not in self.stop_words and w.isalpha()]\n","        matrix = []\n","        for w in text:\n","            try:\n","                matrix.append(self.w2c_model[w])\n","            except Exception as e:\n","                pass\n","        matrix = np.array(matrix)\n","        v = matrix.sum(axis=0)\n","        return v / np.sqrt((v ** 2).sum())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mu8HtPZtZbeM","colab_type":"code","outputId":"7b8b2872-2a02-40e2-a03f-ed79d3ea4b1d","executionInfo":{"status":"ok","timestamp":1544724966610,"user_tz":-60,"elapsed":1243183,"user":{"displayName":"Dimitris Michailidis","photoUrl":"https://lh4.googleusercontent.com/-gqfyDKSRdb4/AAAAAAAAAAI/AAAAAAAAAjM/8OOFAsZoU-M/s64/photo.jpg","userId":"12419194121595599642"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"cell_type":"code","source":["fc = FeatureCreator(train_df)\n","# fc.add_additional_features()\n","fc.add_word2vec_features('GoogleNews-vectors-negative300.bin.gz', 'GoogleNews')\n","print(list(train_df))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in double_scalars\n","/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:698: RuntimeWarning: invalid value encountered in double_scalars\n","  dist = 1.0 - uv / np.sqrt(uu * vv)\n","/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:853: RuntimeWarning: invalid value encountered in double_scalars\n","  dist = np.double(unequal_nonzero.sum()) / np.double(nonzero.sum())\n","/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:1138: RuntimeWarning: invalid value encountered in double_scalars\n","  return l1_diff.sum() / l1_sum.sum()\n"],"name":"stderr"},{"output_type":"stream","text":["['id', 'question1', 'question2', 'is_duplicate', 'GoogleNews_norm_wmd', 'GoogleNews_wmd', 'GoogleNews_cosine_distance', 'GoogleNews_cityblock_distance', 'GoogleNews_jaccard_distance', 'GoogleNews_canberra_distance', 'GoogleNews_euclidean_distance', 'GoogleNews_minkowski_distance', 'GoogleNews_braycurtis_distance', 'GoogleNews_skew_q1vec', 'GoogleNews_skew_q2vec', 'GoogleNews_kur_q1vec', 'GoogleNews_kur_q2vec']\n"],"name":"stdout"}]},{"metadata":{"id":"MONm0v4EyO-8","colab_type":"code","colab":{}},"cell_type":"code","source":["final_train_w2v = train_df[['id', 'GoogleNews_norm_wmd', 'GoogleNews_wmd', 'GoogleNews_cosine_distance', 'GoogleNews_cityblock_distance', 'GoogleNews_jaccard_distance', 'GoogleNews_canberra_distance', 'GoogleNews_euclidean_distance', 'GoogleNews_minkowski_distance', 'GoogleNews_braycurtis_distance', 'GoogleNews_skew_q1vec', 'GoogleNews_skew_q2vec', 'GoogleNews_kur_q1vec', 'GoogleNews_kur_q2vec']]\n","final_train_w2v.to_csv('final_train_w2v.csv', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MT7dhn6Osnax","colab_type":"code","outputId":"bd4edc3e-cff4-46cc-c0c3-37f94c0b11fe","executionInfo":{"status":"ok","timestamp":1544725682342,"user_tz":-60,"elapsed":541743,"user":{"displayName":"Dimitris Michailidis","photoUrl":"https://lh4.googleusercontent.com/-gqfyDKSRdb4/AAAAAAAAAAI/AAAAAAAAAjM/8OOFAsZoU-M/s64/photo.jpg","userId":"12419194121595599642"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"cell_type":"code","source":["fc = FeatureCreator(test_df)\n","fc.add_word2vec_features('GoogleNews-vectors-negative300.bin.gz', 'GoogleNews')\n","print(list(test_df))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in double_scalars\n","/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:698: RuntimeWarning: invalid value encountered in double_scalars\n","  dist = 1.0 - uv / np.sqrt(uu * vv)\n","/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:853: RuntimeWarning: invalid value encountered in double_scalars\n","  dist = np.double(unequal_nonzero.sum()) / np.double(nonzero.sum())\n","/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:1138: RuntimeWarning: invalid value encountered in double_scalars\n","  return l1_diff.sum() / l1_sum.sum()\n"],"name":"stderr"},{"output_type":"stream","text":["['test_id', 'question1', 'question2', 'GoogleNews_norm_wmd', 'GoogleNews_wmd', 'GoogleNews_cosine_distance', 'GoogleNews_cityblock_distance', 'GoogleNews_jaccard_distance', 'GoogleNews_canberra_distance', 'GoogleNews_euclidean_distance', 'GoogleNews_minkowski_distance', 'GoogleNews_braycurtis_distance', 'GoogleNews_skew_q1vec', 'GoogleNews_skew_q2vec', 'GoogleNews_kur_q1vec', 'GoogleNews_kur_q2vec']\n"],"name":"stdout"}]},{"metadata":{"id":"J0DZMsj2anUO","colab_type":"code","colab":{}},"cell_type":"code","source":["final_train_additional = train_df[['id', 'tfidf_word_match', 'tfidf_word_match_stops', 'jaccard_similarity', 'word_count_ratio', 'unique_word_count_ratio', 'total_unique_words']]\n","final_train_additional.to_csv('final_train_additional.csv', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XXlxnRHvsJXy","colab_type":"code","colab":{}},"cell_type":"code","source":["final_test_additional = test_df[['test_id', 'tfidf_word_match', 'tfidf_word_match_stops', 'jaccard_similarity', 'word_count_ratio', 'unique_word_count_ratio', 'total_unique_words']]\n","final_test_additional.to_csv('final_test_additional.csv', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ofJ6PS4KtEW3","colab_type":"code","colab":{}},"cell_type":"code","source":["final_test_w2v = test_df[['test_id', 'GoogleNews_norm_wmd', 'GoogleNews_wmd', 'GoogleNews_cosine_distance', 'GoogleNews_cityblock_distance', 'GoogleNews_jaccard_distance', 'GoogleNews_canberra_distance', 'GoogleNews_euclidean_distance', 'GoogleNews_minkowski_distance', 'GoogleNews_braycurtis_distance', 'GoogleNews_skew_q1vec', 'GoogleNews_skew_q2vec', 'GoogleNews_kur_q1vec', 'GoogleNews_kur_q2vec']]\n","final_test_w2v.to_csv('final_test_w2v.csv', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LUp_bC9j3DqP","colab_type":"text"},"cell_type":"markdown","source":["## Merge all feature files"]},{"metadata":{"id":"t9z4ekw3yiPl","colab_type":"code","colab":{}},"cell_type":"code","source":["final_train_basic_fuzz = pd.read_csv('final_train_basic_fuzz.csv')\n","final_train_additional = pd.read_csv('final_train_additional.csv')\n","final_train_w2v = pd.read_csv('final_train_w2v.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rAcrOVK23OVU","colab_type":"code","colab":{}},"cell_type":"code","source":["final_test_basic_fuzz = pd.read_csv('final_test_basic_fuzz.csv')\n","final_test_additional = pd.read_csv('final_test_additional.csv')\n","final_test_w2v = pd.read_csv('final_test_w2v.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7F_-nTLZ3gL6","colab_type":"code","colab":{}},"cell_type":"code","source":["final_train_features = final_train_basic_fuzz.merge(final_train_additional, on='id')\n","final_train_features = final_train_features.merge(final_train_w2v, on='id')\n","final_train_features.to_csv('final_train_features.csv', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EE5NcW7b479C","colab_type":"code","colab":{}},"cell_type":"code","source":["final_test_features = final_test_basic_fuzz.merge(final_test_additional, on='test_id')\n","final_test_features = final_test_features.merge(final_test_w2v, on='test_id')\n","final_test_features.to_csv('final_test_features.csv', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2U7vjva35_a3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}